{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #c1f2a5\">\n",
    "\n",
    "\n",
    "# PS7\n",
    "\n",
    "In this problem set, we will use the symplest type of recurrent network, and Elman network, to try to learn a symple language.. \n",
    "# Instructions\n",
    "\n",
    "\n",
    "\n",
    "Remember to do your problem set in Python 3. Fill in `#YOUR CODE HERE`.\n",
    "\n",
    "Make sure: \n",
    "- that all plots are scaled in such a way that you can see what is going on (while still respecting specific plotting instructions) \n",
    "- that the general patterns are fairly represented.\n",
    "- to label all x- and y-axes, and to include a title.\n",
    "    \n",
    "    \n",
    "**N.B.** The ideas in this notebook draw heavily from the readings\n",
    "<ol>\n",
    "<li> Elman, J. L. (1990). Finding structure in time. _Cognitive Science, 14_, 179-211.\n",
    "<li>McClelland, J., & Rumelhart, M. (1986). Past tenses of English verbs. In McClelland, J. and Rumelhart, D. (Eds.) _Parallel distributed processing: Explorations in the microstructure of cognition. Vol. 2: Applications_ (pp. 216-271). Cambridge, MA: MIT Press.\n",
    "</ol>\n",
    "<br>\n",
    "If you are confused about some of the ideas in this notebook or would like further clarification, we recommend having a look there.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "One of the most successful (and controversial) applications of neural\n",
    "        networks has been as models of human language. Specifically, contrary to the rules/symbols approach proposed by Chomsky, neural networks have been used to demonstrate that distributed representations can give rise to language learning. You will test whether a\n",
    "simple neural network is capable of learning the rule underlying a\n",
    "context-free language.\n",
    "\n",
    "The language $a^nb^n$, being the set of all strings containing a\n",
    "sequence of $a$'s followed by a sequence of $b$'s of the same length\n",
    "is a simple example of a language that can be generated by a\n",
    "context-free grammar but not a finite-state grammar (because a finite state grammar cannot keep track of the number of times a string has occurred in the sentence). Human languages\n",
    "exhibit similar long-range constraints -- for example, a plural noun\n",
    "at the start of a sentence affects conjugation of a verb at the end,\n",
    "regardless of what intervenes. Some criticisms of applications of\n",
    "neural networks to human languages are based upon their apparent\n",
    "reliance on local sequential structure, which makes them seem much\n",
    "more similar to finite-state grammars than to context-free\n",
    "grammars. In other words, simple neural networks will mostly use the most recent word to predict the next one. An interesting question to explore is thus whether a\n",
    "recurrent neural network can learn to generalize a simple rule\n",
    "characterizing a long-range dependency, such as the rule underlying\n",
    "$a^nb^n$.\n",
    "\n",
    "Recall that an \"Elman\" network, as discussed by Elman (1990), is a\n",
    "recurrent network where the activation of the hidden units at the\n",
    "previous timestep are used as input to the hidden units on the current\n",
    "timestep. This type of network architecture allows the\n",
    "network to learn about sequential dependencies in the input data. In this notebook we will evaluate whether such a network can learn an $a^nb^n$\n",
    "grammar. Here we formalize learning a grammar as being able to correctly\n",
    "predict what the next item in a sequence should be given the\n",
    "rules of the grammar. Therefore, the output node represents the\n",
    "networks's prediction for what the next item in the sequence (the next\n",
    "input) will be -- it outputs a $1$ if it thinks the current input will\n",
    "be followed by an $a$, and outputs a $0$ if it thinks the current\n",
    "input will be followed by a $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Q1. Data\t[5pts, SOLO]\n",
    "We will use the `abdata.npz` dataset for this problem. Make sure that the data file is in the same directory as your notebook while working on the problem set.\n",
    "\n",
    "This dataset has two keys.\n",
    "- The array `train_data` contains the sequence we will use to train our network.\n",
    "- The array `test_data` contains the sequence we will use to evaluate the\n",
    "network.\n",
    "\n",
    "In both `train_data` and `test_data` a $1$ represents an $a$ and a $0$ represents a $b$.\n",
    "\n",
    "`train_data` was constructed by concatenating a randomly ordered\n",
    "set of strings of the form $a^nb^n$, with $n$ ranging from 1 to 11.\n",
    "The frequency of sequences for a given value of $n$ in the training set\n",
    "are given by `np.ceil(50/n)`, thus making them inversely proportional to $n$.\n",
    "The `np.ceil` function returns the smallest integer greater or equal to\n",
    "its input. For example, `np.ceil(3)` is 3, but `np.ceil(3.1)` is\n",
    "4 . `test_data` contains an ordered sequence of strings of the form\n",
    "$a^nb^n$, with $n$ increasing from 1 to 18 over the length of the\n",
    "string.\n",
    "\n",
    "Take a look at the data! You can first print the variables. Next plot the first 100 values of train_data and the first 100 values of test_data in two subplots. Make sure to label the axes and the titles. Upload your plot to gradescope as PS7_Q1.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "ab_data = np.load(\"abdata.npz\")\n",
    "ab_data.keys()\n",
    "\n",
    "# look at train_data\n",
    "print('train_data')\n",
    "train_data = ab_data['train_data']\n",
    "print(train_data[:30])\n",
    "print(len(train_data))\n",
    "\n",
    "## look at test_data\n",
    "print('test_data')\n",
    "test_data = ab_data['test_data']\n",
    "print(test_data)\n",
    "\n",
    "## plot the data\n",
    "#YOUR CODE HERE\n",
    "\n",
    "\n",
    "figure.savefig('PS7_Q1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Q2. Input/output [3 pts HELP]\n",
    "In order to train your network, you will need both training *input* and\n",
    "training *output*. \n",
    "\n",
    "That is, you need a sequence of inputs of the form\n",
    "$a^nb^n$, and a corresponding sequence with the correct output for\n",
    "each item in the input sequence.\n",
    "\n",
    "For this problem we're going to use `train_data[:-1]` as the\n",
    "input training sequence, and `train_data[1:]` as the output\n",
    "training sequence.\n",
    "\n",
    "Explain in gradescope why `train_data[:-1]` and `train_data[1:]` are appropriate input and output\n",
    "sequences. If you're confused by what the sequences\n",
    "`train_data[:-1]` and `train_data[1:]` look like,\n",
    "try creating them in a cell and compare them to `train_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Elman network (provided)\n",
    "We have provided you with a function, `train_Elman`, which takes four arguments:\n",
    "- `input` -- the training input sequence\n",
    "- `output` -- the training output sequence\n",
    "- `num_hidden` -- the number of hidden units\n",
    "- `num_iters` -- the number of training iterations: network needs to be trained on many iterations\n",
    "\n",
    "\n",
    "`train_Elman` will:\n",
    "\n",
    "1) create a network with one input node, the specified number of hidden units, and one output node\n",
    "\n",
    "\n",
    "2) train the network on the training data for the specified number of iterations.\n",
    "\n",
    "\n",
    "The network sees the\n",
    "training data one input at a time (in our case, it sees a single $1$\n",
    "or $0$ per time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Elman(inputs, outputs, num_hidden, num_iters):\n",
    "    \"\"\"\n",
    "    Initializes and trains an Elman network. For details see Elman (1990).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : numpy array\n",
    "        A one dimensional sequence of input values to the network\n",
    "\n",
    "    outputs : numpy array\n",
    "        A one-dimensional sequence of desired output values for each of the\n",
    "        items in inputs\n",
    "\n",
    "    num_hidden : int\n",
    "        The number of hidden units to use in the network\n",
    "\n",
    "    num_iters : int\n",
    "        The number of training iterations to run the network for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    net : dict\n",
    "        Dictionary object containing the trained network weights for each layer. \n",
    "        Key 1 corresponds to the weights from the visibles to the hidden units,\n",
    "        key 2 corresponds to the weights from the hiddens to the output units.\n",
    "\n",
    "    NOTE: Poorly-Python-ported from trainElman.m, which in turn was adapted from\n",
    "    code from http://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/matlab/\n",
    "    recurrent/ which in turn was adapted from Elman (1990) :-)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed=1)\n",
    "\n",
    "    # Parameters\n",
    "    # increment to the derivative of the transfer function (Fahlman's trick)\n",
    "    DerivIncr = 0.2\n",
    "    Momentum  = 0.05\n",
    "    LearnRate = 0.001\n",
    "\n",
    "    num_input  = 1\n",
    "    num_output = 1\n",
    "    num_train  = inputs.shape[0]\n",
    "\n",
    "    if inputs.ndim == 2:\n",
    "        num_input  = inputs.shape[0]\n",
    "        num_output = outputs.shape[0]\n",
    "        num_train  = inputs.shape[1]\n",
    "\n",
    "    if not all([outputs.ndim == inputs.ndim,\n",
    "               inputs.shape[0] == outputs.shape[0]]):\n",
    "        raise ValueError('unequal number of input and output examples')\n",
    "\n",
    "    # create a dictionary to hold the network weights\n",
    "    net = {}\n",
    "    net[1] = np.random.rand(num_hidden, num_input + num_hidden + 1) - 0.5\n",
    "    net[2] = np.random.rand(num_output, num_hidden + 1) - 0.5\n",
    "\n",
    "    # the context layer\n",
    "    # zeros because it is not active when the network starts\n",
    "    Result1 = np.zeros((num_hidden, num_train))\n",
    "\n",
    "    # the row of ones is the bias\n",
    "    Inputs = np.vstack((inputs, np.ones(num_train)))\n",
    "    Desired = outputs\n",
    "\n",
    "    delta_w1 = 0.\n",
    "    delta_w2 = 0.\n",
    "\n",
    "    # Training\n",
    "    for ii in range(num_iters):\n",
    "        # Recurrent state\n",
    "        # includes current inputs, as well as the output of the hidden layer\n",
    "        # from the previous time step\n",
    "        Input1 = np.vstack((Inputs, np.hstack([np.zeros((num_hidden,1)),  Result1[:,:-1]])))\n",
    "\n",
    "        # Forward propagate activations\n",
    "        # input --> hidden\n",
    "        NetIn1 = np.dot(net[1], Input1)\n",
    "        Result1 = np.tanh(NetIn1)\n",
    "\n",
    "        # Hidden --> output\n",
    "        # we again add a row of ones for bias\n",
    "        Input2 = np.vstack((Result1, np.ones(num_train)))\n",
    "        NetIn2 = np.dot(net[2], Input2)\n",
    "        Result2 = np.tanh(NetIn2)\n",
    "\n",
    "        # Backprop errors\n",
    "        # output --> hidden\n",
    "        Result2Error = Result2 - Desired\n",
    "        In2Error = Result2Error * (DerivIncr + np.cosh(NetIn2)**(-2))\n",
    "\n",
    "        # hidden --> input\n",
    "        Result1Error = np.dot(net[2].T, In2Error)\n",
    "        In1Error = Result1Error[:-1, :] * (DerivIncr + np.cosh(NetIn1)**(-2))\n",
    "\n",
    "        # Calculate weight updates\n",
    "        dw2 = np.dot(In2Error, Input2.T)\n",
    "        dw1 = np.dot(In1Error, Input1.T)\n",
    "\n",
    "        delta_w2 = -LearnRate * dw2 + Momentum * delta_w2\n",
    "        delta_w1 = -LearnRate * dw1 + Momentum * delta_w1\n",
    "\n",
    "        net[2] = net[2] + delta_w2\n",
    "        net[1] = net[1] + delta_w1\n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 - learn anbn language [HELP 5 pts]\n",
    "Complete the function `anbn_learner` below to train an \"Elman\"\n",
    "network with two hidden units using the provided function `train_Elman` (remember\n",
    "to use the input **train_data[:-1]** and output **train_data[1:]** sequences from Q2).\n",
    "\n",
    "\n",
    "\n",
    "Train the network for *100 iterations*, and return the final output of the network.\n",
    "We provide test cases. If you got the function right, the following cell should print \"Success\". Otherwise, it will give you an error message that will help with debugging.\n",
    "\n",
    "Copy your code into gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c780e1b1346209cf913d01a4babea476",
     "grade": false,
     "grade_id": "anbn_learner",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def anbn_learner(train_data):\n",
    "    \"\"\"\n",
    "    Creates an \"Elman\" neural network with two hidden units and trains it\n",
    "    on the provided data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: numpy array of shape (n,)\n",
    "        the data on which to train the Elman network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    net: dictionary with 2 keys\n",
    "        a dictionary containing the weights of the network. Valid keys are 1 and 2. \n",
    "        key 1 is for the weights between the input and the hidden units, and \n",
    "        key 2 is for the weights between the hidden units and the output units.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8474acbd70334d48c94b2a3f9c2ddd76",
     "grade": true,
     "grade_id": "check_anbn_learner",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that anbn_learner returns the correct output\"\"\"\n",
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal, assert_almost_equal \n",
    "\n",
    "# check that abdata hasn't been modified\n",
    "ab = np.load(\"data/abdata.npz\")\n",
    "assert_array_equal(test_data, ab['test_data'], \"test_data array has changed\")\n",
    "assert_array_equal(train_data, ab['train_data'], \"train_data array has changed\")\n",
    "\n",
    "# generate test data\n",
    "traindata = np.zeros(20)\n",
    "traindata[10:] = 1.\n",
    "\n",
    "net = anbn_learner(traindata)\n",
    "\n",
    "# check that net has the correct shape and type\n",
    "assert_equal(type(net), dict, \"net should be a dict of network weights\")\n",
    "assert_equal(len(net), 2, \"incorrect number of layers in net\")\n",
    "assert_equal(list(net.keys()), [1,2], \"keys for net should be 1 and 2\")\n",
    "\n",
    "# check the dimensions of the weight matrices\n",
    "assert_equal(net[1].shape, (2,4), \"invalid network weights for the input -> hidden layer\")\n",
    "assert_equal(net[2].shape, (1,3), \"invalid network weights for the hidden -> output layer\")\n",
    "\n",
    "# check the weight matrix sums to the correct value on testdata\n",
    "assert_almost_equal(np.sum(net[1]), -1.9326, places=4, msg=\"weights for input --> hidden layer are incorrect\")\n",
    "assert_almost_equal(np.sum(net[2]), 0.01825, places=4, msg=\"weights for hidden --> output layer are incorrect\")\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Q4 - checking the trained network [5 pts, solo]\n",
    "Once the network is trained (*your anbn_learner function should pass the test case in the previous cell)*, you can test it on a new set of sequences\n",
    "and evaluate its predictions to see how well it has learned the target\n",
    " grammar. To generate predictions from the trained network, we use the provided function `predict_Elman`. \n",
    " \n",
    "Use your `anbn_learner` function on `train_data` to train a network, then use your trained network (by passing it as the first input to `predict_Elman` function) to predict the sequences in `test_data`. \n",
    "\n",
    "A) The `predict_Elman` function returns an array of predicted values with the same dimensions as the input.Plot your prediction (sequence index number on x axis, predicted values on y axis) and upload the figure to gradescope as PS7_Q4.png.\n",
    "\n",
    "B) Look back at your plot of the test data in Q1 - what do you think of the network's predictions? Do your predictions approximate the testing data? Explain why you think they do, or why you think they do not, by referring to the mechanisms of recurrent networks and the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_Elman(net, inputs):\n",
    "    \"\"\"\n",
    "    Uses the Elman network parameterized by the weights in net to generate\n",
    "    predictions for the elements in inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net : dict\n",
    "        Dictionary object containing the trained network weights and\n",
    "        recurrent connections as produced by train_Elman. Key 1 corresponds \n",
    "        to the weights from the visibles to the hidden units, key 2 \n",
    "        corresponds to the weights from the hiddens to the output units.\n",
    "\n",
    "    inputs : numpy array\n",
    "        A one dimensional sequence of input values to the network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outputs : numpy array\n",
    "        An array containing the predictions generated by the Elman network for the\n",
    "        items in inputs.\n",
    "        \n",
    "    NOTE: Poorly-Python-ported from predictElman.m, which in turn was adapted from\n",
    "    code from http://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/matlab/\n",
    "    recurrent/ which in turn was adapted from Elman (1990) :-)\n",
    "    \"\"\"\n",
    "    num_output = 1\n",
    "    num_hidden = net[1].shape[0]\n",
    "    num_train  = inputs.shape[0]\n",
    "\n",
    "    if inputs.ndim == 2:\n",
    "        num_train = inputs.shape[1]\n",
    "\n",
    "    Inputs = np.vstack((inputs, np.ones([1, num_train])))\n",
    "    Result1 = np.zeros([num_hidden, 1])\n",
    "\n",
    "    outputs = np.zeros(num_train)\n",
    "\n",
    "    for i in range(num_train):\n",
    "        Input1 = np.append(Inputs[:, i], Result1)\n",
    "        NetIn1 = np.dot(net[1], Input1)\n",
    "        Result1 = np.tanh(NetIn1)\n",
    "\n",
    "        Input2 = np.append(Result1, np.ones((1, 1)))\n",
    "        NetIn2 = np.dot(net[2], Input2)\n",
    "        outputs[i] = np.tanh(NetIn2)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "figure.savefig('PS7_Q4.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Q5 - Quantifying the model performance [2pts, HELP]\n",
    "\n",
    "How well does the network do at predicting the next letter? Has it learned the language? Let's look more carefully.\n",
    "\n",
    "To quantify how well the network performs we are going to look at how much the predicted sequence deviates from expectations. The squared error (SE) for a prediction $p_i$ in the prediction vector ${\\bf p}$\n",
    "compared to a target value ${y_i}$ in the target vector ${\\bf y}$ is\n",
    "\n",
    "\\begin{equation}\n",
    "SE_i = (p_i-y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "That is, the squared error is just the squared difference between the\n",
    "predicted and target value.\n",
    "\t\n",
    "Complete the function `squared_error`, which takes in an array of test data and an array of\n",
    "predictions. The function should return an error array **with the same number of elements as the test data**, containing the SE for each\n",
    "of the predictions of the network compared against the corresponding value in `test_data`. \n",
    "\n",
    "Remember that the predictions refer to the _next_ item in the sequence\n",
    "(e.g.  `predictions[0]` should be compared to\n",
    "`test_data[1]`, etc.). You should append an $a$ (coded as a $1$) to the end of your test data to equate the array sizes (describing the start of a new sequence of $a^nb^n$). \n",
    "\n",
    "We have provided test cases again in the cell below. If your function is right, it should print success. Otherwise, it will give you an error with feedback.\n",
    "\n",
    "Copy your function into gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc34fc992d440beb6bea797faabbc3e5",
     "grade": false,
     "grade_id": "sq_err",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def squared_error(predictions, test_data):\n",
    "    \"\"\"\n",
    "    Uses equation 1 to compute the SE for each of the predictions made \n",
    "    by the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions: numpy array of shape (n,)\n",
    "        an array of predictions from the Elman network\n",
    "    \n",
    "    test_data: numpy array of shape (n,) \n",
    "        the array of test data from which predictions were generated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    se_vector: numpy array of shape (n,)\n",
    "        an array containing the SE for each of items in predictions \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR OWN TESTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b208daede991b158bc6b80965cd889e0",
     "grade": true,
     "grade_id": "check_sq_err",
     "locked": false,
     "points": 1.25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that squared_error returns the correct output\"\"\"\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# generate test data\n",
    "pred = np.array([1, 0, 1])\n",
    "test = np.array([0, 1, 0])\n",
    "se = squared_error(pred, test)\n",
    "\n",
    "# check that squared_error returns the correct output for testdata\n",
    "#assert_equal(se.dtype, np.float64, \"squared_error should return an array of floats\")\n",
    "assert_equal(se.shape, (3,), \"squared_error returned an array of the incorrect size on the validate testdata\")\n",
    "assert_array_equal(se, np.zeros(3), \"squared_error should return all zeros on the validate testdata\")\n",
    "\n",
    "# check that squared_error compares the correct elements\n",
    "pred = np.zeros(1)\n",
    "test = np.zeros(1)\n",
    "se = squared_error(pred, test)\n",
    "assert_equal(se, np.ones(1), \"squared_error([0],[0]) should have returned a 1 (did you remember to append an a to testdata?\")\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Q6 [5 pts, SOLO]\n",
    "Train the network on the train_data, then apply it to the test_data: try to predict test data using the weights of the trained network. Measure the resulting squared error.\n",
    "\n",
    "Use matplotlib to plot a bar graph of the squared error for each training example. You should have the sequence iteration number on the x axis, and the error values on y axis. Don't forget to provide a title and labels your $x$ and $y$ axes!\n",
    "\n",
    "If you have difficulty interpreting this graph, you may want to\n",
    "examine a few of the values in `test_data`, `predictions`, and your `mse_vector` to see how they are related.\n",
    "Upload your figure PS7_Q6.png to gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "361a23caac2e11f8082d5b4f29f2e4be",
     "grade": false,
     "grade_id": "plot_error",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# create the figure\n",
    "fig, axis = plt.subplots()\n",
    "axis.set_xlim([0.0, 350.0]), axis.set_ylim([0.0,.7])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "axis.bar(np.arange(len(se_vector)),se_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Q7 [SOLO, 5pts]\n",
    "\n",
    "To get a better idea of what is going on, let's have a look at the specific values in `test_data` where the prediction error spikes. Use the provided code below and look at its output. \n",
    "\n",
    "At which points in test_data do the large errors occur? Explain which parts of the data the network is predicting ok, and which parts it's not predicting well. Answer in gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# prints the 3 values preceding and 2 values following the spot where \n",
    "# the prediction error >= 0.5\n",
    "error_spike_idxs = np.argwhere(se_vector >= 0.5) + 1\n",
    "error_spike_idxs = error_spike_idxs[:-1]\n",
    "\n",
    "for i in error_spike_idxs:\n",
    "    print('3 values preceding MSE spike: {}\\tValue at MSE spike: {}'\n",
    "          '\\t\\t2 values following MSE spike: {}'\\\n",
    "          .format(test_data[i[0]-3:i[0]], test_data[i[0]], test_data[i[0]+1:i[0]+3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "\n",
    "\n",
    "## Q8.1. Conclusions [SOLO 5 pts]\n",
    "\n",
    "Earlier we said that we can evaluate whether the network has learned\n",
    "the grammar by looking at the predictions it makes. If the network has\n",
    "learned the $a^nb^n$ grammar, in what cases should it make correct\n",
    "predictions? When should it make incorrect predictions?\n",
    "\n",
    "Do your predictions about when the network should make correct/incorrect predictions if it has learned the $a^nb^n$ grammar match the the times when the network makes large errors, as identified in Q7? Did the network learn the $a^nb^n$ language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Q8.2. Conclusion [SOLO 5pts]\n",
    "At what level of the Chomsky hierarchy is the $a^nb^n$ grammar? \n",
    "\n",
    "How does this compare to the level of most natural languages? Specifically, what level of Chomsky's hierarchy describes most of the natural languages, and are these levels higher relative to that of $a^nb^n$?\n",
    "\n",
    "Use this to explore the implications of your results from Q7 and Q8.1 for using the Elman network to model the relationships present in human language. Is the Elman network likely to be sufficient to capture human language satisfactorily?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #c1f2a5\">\n",
    "\n",
    "# Submission\n",
    "\n",
    "### <span style=\"color:red\">Attention! Code submission requirement!</span>   \n",
    "    \n",
    "When you're done with your problem set, do the following:\n",
    "- Upload your answers in Gradescope's PS7.\n",
    "- Convert your Jupyter Notebook into a `.py` file by doing so:    \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "<center>    \n",
    "  <img src=\"https://www.dropbox.com/s/7s189m4dsvu5j65/instruction.png?dl=1\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "<div style=\"background-color: #c1f2a5\">\n",
    "    \n",
    "- Submit the `.py` file you just created in Gradescope's PS7-code.\n",
    "    \n",
    "</div>        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
