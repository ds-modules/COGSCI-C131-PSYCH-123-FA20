{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color: #c1f2a5\">\n",
    "\n",
    "\n",
    "# PS12\n",
    "\n",
    "In this problem set, we are going to learn to navigate a simple maze.\n",
    "    \n",
    "# Instructions\n",
    "\n",
    "\n",
    "\n",
    "Remember to do your problem set in Python 3. Fill in `#YOUR CODE HERE`.\n",
    "\n",
    "Unless we specify otherwise, make sure: \n",
    "- that all plots are scaled in such a way that you can see what is going on (while still respecting specific plotting instructions) \n",
    "- that the general patterns are fairly represented.\n",
    "- to label all x- and y-axes, and to include a title.\n",
    "    \n",
    "**Test cases are here to help you debug your code, but passing them successfully is not a guarantee that your code is correct.**\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to learn to navigate a simple maze. In this maze, you have to make two successive left-right decisions (0 or 1). The first takes you from state $s=0$ to states $s=1$ or $s=2$. The second decision takes you to states $3-6$, as pictured below.\n",
    "![](Maze_JDS.png)\n",
    "Taking a first step to S1 or S2 is costly ($r=-1$), but can get you to a position to get reward (states 3 and 6 give you 4, and 10 points respectively). Your goal is to gain as many points as possible, so you should learn to select right-right, which will give you a total cumulated reward of $-1+10 = 9$.\n",
    "\n",
    "In this problem set, we will try a few different algorithms to learn this, and see how they behave differently. \n",
    "\n",
    "Below, we provide the reward function and the transition function.\n",
    "- R[i,j] indicates what reward you obtain for selecting action j in state i\n",
    "- T[i,j] indicates what state you will be in after you select action j in state i\n",
    "Test it out to make sure it's right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([[-1,-1],[4,-1],[-1,10]])\n",
    "T = np.array([[1,2],[3,4],[5,6]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Softmax choice [HELP, 2pts]\n",
    "\n",
    "In Q1, you will:\n",
    "- write softmax function according to the specifications below\n",
    "- use the softmax function to figure out how much cumulated reward in average if choosing randomly\n",
    "\n",
    "As a reminder, here is the softmax equation: \n",
    "$$ P(a|s) = \\frac{exp(\\beta Q(s,a)}{\\sum_i exp(\\beta Q(s,a_i))}$$\n",
    "\n",
    "To limit overflow issues, we recommend rewriting this equation like this:\n",
    "$$ P(a|s) = \\frac{1}{\\sum_i exp(\\beta [Q(s,a_i) - Q(s,a)])}$$\n",
    "\n",
    "Check that your softmax function is correct with the tests, and report in gradescope the output of the next cell.\n",
    "\n",
    "In that cell, we've provided code for an agent that doesn't learn, and just picks left or right based on their bias (which one they prefer). Make sure to read through the function, as it will help you code more intelligent agents later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(beta,Qs):\n",
    "    \"\"\"\n",
    "    Returns softmax probability of a choice.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : real number\n",
    "        The softmax inverse temperature parameter\n",
    "    Qs: a (1,n) numpy array of values\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    a: an integer in [0,n]\n",
    "        a choice made with probability defined by softmax(betaQs)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your own tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Checksoftmax computes the correct values\"\"\"\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "assert_allclose(softmax(100,np.array([0,1])), 1.0)\n",
    "assert_allclose(softmax(100,np.array([0,1,0])), 1.0)\n",
    "assert_allclose(softmax(100,np.array([0,0,1])), 2.0)\n",
    "assert_allclose(softmax(100,np.array([1,0,0,0])), 0.0)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_right_bias(beta,Qs,n_trials):\n",
    "    \"\"\"\n",
    "    Returns average cumulated reward for a left-right bias agent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : real number\n",
    "        The softmax inverse temperature parameter\n",
    "    Qs: a (1,n) numpy array of values\n",
    "    n_trials: integer - number of trials to average over\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        average cumulated reward\n",
    "    \"\"\"\n",
    "    Cum_R = np.zeros(n_trials)\n",
    "    for i in range(n_trials):\n",
    "        # start at state 0\n",
    "        initial_state = 0\n",
    "        # make the first choice\n",
    "        first_choice = softmax(beta,Qs)\n",
    "        # see what reward that state gives, and what the next state is\n",
    "        first_reward = R[initial_state,first_choice]\n",
    "        next_state = T[initial_state,first_choice]\n",
    "        # make a second choice\n",
    "        second_choice = softmax(beta,Qs)\n",
    "        # get reward\n",
    "        second_reward =  R[next_state,second_choice]\n",
    "        # store cumulated reward in this trial\n",
    "        Cum_R[i] =first_reward + second_reward\n",
    "    # return average cumulated reward.    \n",
    "    return np.mean(Cum_R)\n",
    "    \n",
    "n_trials = 10000\n",
    "beta = 0\n",
    "Qs = np.zeros(2)\n",
    "print('Random choice leads to average cumulated reward of: '+str(np.around(left_right_bias(beta,Qs,n_trials),decimals=2)))\n",
    "beta = 5\n",
    "Qs = np.array([.8,.5])\n",
    "print('Random left-biased choice leads to average cumulated reward of: '+str(np.around(left_right_bias(beta,Qs,n_trials),decimals=2)))\n",
    "beta = 5\n",
    "Qs = np.array([.5,.8])\n",
    "print('Random right-biased choice leads to average cumulated reward of: '+str(np.around(left_right_bias(beta,Qs,n_trials),decimals=2)))\n",
    "beta = 50\n",
    "Qs = np.array([.5,.8])\n",
    "print('Near-greedy right-biased choice leads to average cumulated reward of: '+str(np.around(left_right_bias(beta,Qs,n_trials),decimals=2)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. SARSA \n",
    "\n",
    "Now, we're going to code an agent that actually learns the values of different choices. \n",
    "We'll use the SARSA equation:\n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha (r_t + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t))$$\n",
    "\n",
    "If there's no next state/action (i.e. when the algorithm reaches the end of the maze in any of states 3-6), the equation is simply: \n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha (r_t  - Q(s_t,a_t))$$\n",
    "\n",
    "\n",
    "### Q2.1 SARSA Update [3 pts, HELP] \n",
    "Complete the `sarsa` function below that takes the learning rate parameter $\\alpha$, the discount factor $\\gamma$; the Q-value table (a (3,2) np array); as well as a sequence of two successive states, two successive actions, and two successive outcomes describing one path through the maze. The function should return the updated Q-value table for states 0,1,2 and actions 0,1.\n",
    "\n",
    "Copy paste your code in gradescope (or upload a screenshot if it doesn't work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(alpha, gamma,Q,s,a,r):\n",
    "    \"\"\"\n",
    "    Returns updated Q-table.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : real number\n",
    "        The learning rate parameter\n",
    "    gamma : real number\n",
    "        The discount parameter\n",
    "    Q: a (3,2) numpy array of values for states (0,1,2) and actions (0,1)\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    Q: a (3,2) numpy array of updated values for states (0,1,2) and actions (0,1)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "   #YOUR CODE HERE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add test cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check sarsa computes the correct values\"\"\"\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "s = np.array([0,1]).astype(int)\n",
    "a = np.zeros(2).astype(int)\n",
    "r = np.array([-1,4]).astype(int)\n",
    "\n",
    "alpha = .1\n",
    "gamma=.9\n",
    "assert_allclose(sarsa(alpha, gamma,0.5*np.ones([3,2]),s,a,r), np.array([[0.395,0.5],[0.85,0.5],[0.5,0.5]]))\n",
    "a = np.ones(2).astype(int)\n",
    "assert_allclose(sarsa(alpha, gamma,0.5*np.ones([3,2]),s,a,r), np.array([[0.5,0.395],[0.5,0.85],[0.5,0.5]]))\n",
    "s = np.array([0,2]).astype(int)\n",
    "assert_allclose(sarsa(alpha, gamma,0.5*np.ones([3,2]),s,a,r), np.array([[0.5,0.395],[0.5,0.5],[0.5,0.85]]))\n",
    "alpha = .2\n",
    "assert_allclose(sarsa(alpha, gamma,0.5*np.ones([3,2]),s,a,r), np.array([[0.5,0.29],[0.5,0.5],[0.5,1.2]]))\n",
    "Q= sarsa(alpha, gamma,0.5*np.ones([3,2]),s,a,r)\n",
    "gamma=.75\n",
    "assert_allclose(sarsa(alpha, gamma,Q,s,a,r), np.array([[0.5,0.212],[0.5,0.5],[0.5,1.76]]))\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 One trial SARSA [5 pts, SOLO] \n",
    "\n",
    "Now, use the `sarsa` function you wrote as well as the `softmax` function you wrote to complete the function `onetrial_sarsa` below. This function should navigate one path through the maze, and return the two choices and rewards experienced, as well as the updated Q-value. Hint: you can use the `left_right_bias` function as a model for walking through the maze. \n",
    "\n",
    "Upload the figure produced by the following cell. Make sure to try plotting multiple times, and to upload the most representative figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onetrial_sarsa(parameters,Q,R,T):\n",
    "    \"\"\"\n",
    "    Returns updated Q-table.\n",
    "    \n",
    "    Input\n",
    "    ----------\n",
    "    parameters : (1,3) numpy array\n",
    "        model parameters (beta, alpha, gamma)\n",
    "    Q: a (3,2) numpy array of values for states (0,1,2) and actions (0,1)\n",
    "    R: reward function (3,2) numpy array\n",
    "    T: transition function (3,2) numpay array\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    Q: (3,2) numpy array of updated Q-values\n",
    "    a: a (1,2) numpy array of the sequence of two choices\n",
    "    r: a (1,2) numpy array of the sequence of two rewards.\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your own test cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results \n",
    "\n",
    "R = np.array([[-1,-1],[4,-1],[-1,10]])\n",
    "T = np.array([[1,2],[3,4],[5,6]])\n",
    "\n",
    "\n",
    "nTrials = 100\n",
    "Qs = np.empty((6,nTrials))\n",
    "Q = np.array([[.5,.5],[.5,.5],[.5,.5]])\n",
    "beta = 5\n",
    "alpha = .1\n",
    "gamma = 1\n",
    "parameters = np.array([beta,alpha,gamma])\n",
    "\n",
    "for t in range(nTrials):\n",
    "    newQ,a,r=onetrial_sarsa(parameters,Q,R,T)\n",
    "    Qs[:,t] = np.ndarray.flatten(newQ)\n",
    "    \n",
    "figure, axis = plt.subplots()\n",
    "for i in range(6):\n",
    "    axis.plot(Qs[i,:])\n",
    "    \n",
    "axis.legend(['Q(S0,A0)','Q(S0,A1)','Q(S1,A0)','Q(S1,A1)','Q(S2,A0)','Q(S2,A1)'])\n",
    "plt.title('One trial SARSA')\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "fig.savefig('PS12_Q2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Performance\n",
    "\n",
    "Now, we'll run multiple simulations (100), and see what the final Q-values are. Use the provided parameter values (50 trials, beta=5, alpha=0.1, gamma=1). Make sure to re-initialize your Q-values for each simulation! \n",
    "\n",
    "\n",
    "### Q3.1 Plot final Q-values [5 pts, SOLO]\n",
    "In three subplots, plot scatterplots of a) Q(s0,a0) vs. Q(s0,a1) b) Q(s0,a1) vs. Q(s1,a0) and c) Q(s2,a2).\n",
    "\n",
    "Upload your graph to Bcourses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R = np.array([[-1,-1],[4,-1],[-1,10]])\n",
    "T = np.array([[1,2],[3,4],[5,6]])\n",
    "\n",
    "\n",
    "nTrials = 50\n",
    "niterations = 100\n",
    "Qs = np.empty((6,niterations))\n",
    "beta = 5\n",
    "alpha = .1\n",
    "gamma = 1\n",
    "parameters = np.array([beta,alpha,gamma])\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "fig.savefig('PS12_Q3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q3.2 Interpretation [3 pts, HELP]\n",
    "\n",
    "If you got the previous function right, you should observe a small cluster of simulations that end up with different Q-values than the main cluster of simulations, at the same time for more than one state-action pair. Explain in gradescope what happened in those simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q4. Parameter effects\n",
    "\n",
    "In this part, we will explore the role of the various parameters on learning performance. To summarize performance, we will compute the average cumulated reward obtained at each trial (It should be a number between -2, the minimum reward over two steps, and 10-1=9, the maximum reward over two steps). \n",
    "\n",
    "Complete the function `cumulative_outcome` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_outcome(parameters,R,T,nTrials,niterations):\n",
    "    \"\"\"\n",
    "    Returns average cumulative earnings.\n",
    "    \n",
    "    Input\n",
    "    ----------\n",
    "    parameters : (1,3) numpy array\n",
    "        model parameters (beta, alpha, gamma)\n",
    "    R: reward function (3,2) numpy array\n",
    "    T: transition function (3,2) numpay array\n",
    "    nTrials: integer (number of learning trials in an iteration)\n",
    "    niterations: number of iterations of the simulation\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    average cumulated reward for two steps, over trials and simulations.\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.1 Effect of beta [3 pts, SOLO]\n",
    "\n",
    "to investigate the effect of beta on performance, use $\\gamma = 5$. In one figure, plot performance as a function of beta value ranging 1-20 in increments of 2, for $\\alpha = 0.1$ and $\\alpha=0.3$. Add a legend. \n",
    "\n",
    "### Q4.2 Effect of beta - interpretation [4 pts, HELP]\n",
    "What do you observe?  Explain how beta affects performance in both cases, and explain why, including any differences you may observe.\n",
    "\n",
    "If your graph is wrong/you don't have a graph, you can still try to explain in general terms what effects $\\beta$ could have on performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nTrials=20\n",
    "niterations=1000\n",
    "\n",
    "gamma=1\n",
    "\n",
    "\n",
    "betaline=np.arange(1,20,2)\n",
    "figure, axis = plt.subplots()\n",
    "\n",
    "alpha=.1\n",
    "R_beta = np.empty(len(betaline))\n",
    "\n",
    "##YOUR CODE HERE\n",
    "\n",
    "fig.savefig('PS12_Q4_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.3 Effect of alpha [SOLO, 3pts]\n",
    "\n",
    "To investigate the effect of $\\alpha$ on performance, use $\\gamma = 5$. In one figure, plot performance as a function of alpha value ranging 0.05 to 1 in increments of .05, for $\\beta = 1$, and $\\beta = 5$. Add a legend. \n",
    "\n",
    "### Q4.4 Effect of alpha [HELP, 4pts]\n",
    "\n",
    "What do you observe?  Explain how $\\alpha$ affects performance in different cases, and explain why, including any differences you may observe.\n",
    "\n",
    "If your graph is wrong/you don't have a graph, you can still try to explain in general terms what effects $\\alpha$ could have on performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nTrials=20\n",
    "niterations=1000\n",
    "\n",
    "gamma=1\n",
    "alphaline=np.arange(.05,1,.05)\n",
    "\n",
    "\n",
    "##YOUR CODE HERE\n",
    "\n",
    "\n",
    "fig.savefig('PS12_Q4_4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Planning \n",
    "Now, we will try model-based reinforcement learning.\n",
    "In this algorithm, the agent knows the reward function and the transition function. They use it to plan ahead: mentally walk through all possible paths in the maze, evaluate the cumulative reward for each path. They use this cumulative reward value between each path to choose one of the (four) possible paths, via a softmax function.\n",
    "\n",
    "\n",
    "### 5.1 Implementing planning [3 pts, HELP]\n",
    "Complete the `onetrial_planning` function below. Your function should a) compute the cumulative reward for each path of the tree and return it b) choose between the four possible paths using a softmax c) return the sequence of two actions and two rewards obtained.\n",
    "\n",
    "Run your function 1000 times with $\\beta=0.5$, $\\gamma=0.5$, and plot as a bar plot the proportion of times each of the four paths was chosen. Upload your figure to bcourses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onetrial_planning(parameters,R,T):\n",
    "    \"\"\"\n",
    "    Runs one trial of planning.\n",
    "    \n",
    "    Input\n",
    "    ----------\n",
    "    parameters : (1,2) numpy array\n",
    "        model parameters (beta, gamma)\n",
    "    R: reward function (3,2) numpy array\n",
    "    T: transition function (3,2) numpay array\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    Q: (1,4) numpy array of cumulative reward for each path\n",
    "    a: (1,2) numpy array of the sequence of two choices selected\n",
    "    r: (1,2) numpy array of the sequence of two rewards obtained.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your own tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Check sarsa computes the correct values\"\"\"\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "assert_allclose(onetrial_planning(np.array([1,1]),np.array([[-1,-1],[4,-1],[-1,10]]),T)[0], np.array([3,-2,-2,9]))\n",
    "assert_allclose(onetrial_planning(np.array([1,.5]),np.array([[-1,-1],[4,-1],[-1,10]]),T)[0], np.array([1,-1.5,-1.5,4]))\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "    \n",
    "fig.savefig('PS12_Q5_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.2 Comparing planning to learning [5 pts, HELP]\n",
    "We're going to compare Sarsa and Planning. Planning has more information, so it should perform better (i.e. get more cumulative reward). But it should also be more computationally expensive. We'll measure that by the time it takes the program to run.\n",
    "\n",
    "Run both Sarsa and Planning for 1000 iterations of 100 trials each. In two subplots, plot a) the average cumulative reward for each model (x-axis), b) the duration of the computation for each model. [HINT: use the timeit package.]\n",
    "\n",
    "Do the results match the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "# YOUR CODE HERE\n",
    "fig.savefig('PS12_Q5_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"background-color: #c1f2a5\">\n",
    "\n",
    "# Submission\n",
    "\n",
    "When you're done with your problem set, do the following:\n",
    "- Upload your answers in Gradescope's PS12.\n",
    "\n",
    "- Convert your Jupyter Notebook into a `.py` file by doing so:    \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "<center>    \n",
    "  <img src=\"https://www.dropbox.com/s/7s189m4dsvu5j65/instruction.png?dl=1\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "<div style=\"background-color: #c1f2a5\">\n",
    "    \n",
    "- Submit the `.py` file you just created in Gradescope's PS5-code.\n",
    "    \n",
    "</div>        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
